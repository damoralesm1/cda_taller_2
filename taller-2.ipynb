{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19d244e6-f45d-447d-9526-0044066f331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install tensorflow\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Taller 2: Implementación de Machine Learning en un Supermercado Inteligente\n",
    "\n",
    "### Estudiante: Diego Alvaro Morales Medrano\n",
    "### Código: 202315708\n",
    "\n",
    "## Introducción\n",
    "\n",
    "El objetivo de este taller es desarrollar un modelo de Machine Learning que permita identificar productos en un supermercado inteligente. Esto facilitará a los clientes tomar productos y salir sin pasar por caja, ya que las cámaras y sensores registrarán automáticamente los artículos seleccionados.\n",
    "\n",
    "En este taller, abordaremos los siguientes puntos:\n",
    "\n",
    "1. Entendimiento y preparación de los datos.\n",
    "2. Entrenamiento de dos modelos de Machine Learning.\n",
    "3. Análisis de resultados y selección del mejor modelo.\n",
    "4. Cálculo del valor generado y ROI.\n",
    "5. Presentación de insights y recomendaciones.\n",
    "6. Clasificación detallada por tipos de productos y sus marcas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation",
   "metadata": {},
   "source": [
    "## 1. Entendimiento y Preparación de los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-libraries",
   "metadata": {},
   "source": [
    "### 1.1 Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "load-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense,\n",
    "                                     Dropout, BatchNormalization)\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download-dataset",
   "metadata": {},
   "source": [
    "### 1.2 Descarga y Preparación del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "download-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'GroceryStoreDataset'...\n",
      "remote: Enumerating objects: 6559, done.\u001b[K\n",
      "remote: Counting objects: 100% (266/266), done.\u001b[K\n",
      "remote: Compressing objects: 100% (231/231), done.\u001b[K\n",
      "remote: Total 6559 (delta 45), reused 37 (delta 35), pack-reused 6293 (from 1)\u001b[K\n",
      "Receiving objects: 100% (6559/6559), 116.26 MiB | 1.30 MiB/s, done.\n",
      "Resolving deltas: 100% (275/275), done.\n",
      "Updating files: 100% (5717/5717), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/marcusklasson/GroceryStoreDataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "### 1.3 Cargando el Dataset\n",
    "\n",
    "El dataset se encuentra en la carpeta `GroceryStoreDataset` y está estructurado de la siguiente manera:\n",
    "\n",
    "- `train.txt`, `val.txt`, `test.txt`: Archivos que contienen las rutas de las imágenes y sus etiquetas.\n",
    "- `classes.csv`: Archivo que contiene los nombres de las clases y sus etiquetas correspondientes.\n",
    "\n",
    "Vamos a cargar estos archivos y explorar el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "load-dataframes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al directorio de datos\n",
    "data_dir = './GroceryStoreDataset/dataset'\n",
    "\n",
    "# Cargar clases\n",
    "classes_df = pd.read_csv(os.path.join(data_dir, 'classes.csv'))\n",
    "classes_df.columns = ['class_name', 'class_id', 'coarse_class_name', 'coarse_class_id', 'class_image', 'product_description']\n",
    "\n",
    "# Crear diccionarios para mapear etiquetas a nombres\n",
    "fine_label_to_name = dict(zip(classes_df['class_id'], classes_df['class_name']))\n",
    "coarse_label_to_name = dict(zip(classes_df['coarse_class_id'], classes_df['coarse_class_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-exploration",
   "metadata": {},
   "source": [
    "### 1.4 Exploración de Clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "explore-classes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: (2640, 3)\n",
      "Tamaño del conjunto de validación: (296, 3)\n",
      "Tamaño del conjunto de prueba: (2485, 3)\n"
     ]
    }
   ],
   "source": [
    "# Función para cargar los datos desde los archivos txt\n",
    "def load_data(txt_file):\n",
    "    data = []\n",
    "    with open(os.path.join(data_dir, txt_file), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split(\",\")\n",
    "            image_path = parts[0]\n",
    "            fine_label = int(parts[1])\n",
    "            coarse_label = int(parts[2])\n",
    "            data.append({'image_path': os.path.join(data_dir, image_path),\n",
    "                         'fine_label': fine_label,\n",
    "                         'coarse_label': coarse_label})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Cargar conjuntos de datos\n",
    "train_df = load_data('train.txt')\n",
    "val_df = load_data('val.txt')\n",
    "test_df = load_data('test.txt')\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", train_df.shape)\n",
    "print(\"Tamaño del conjunto de validación:\", val_df.shape)\n",
    "print(\"Tamaño del conjunto de prueba:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "select-categories",
   "metadata": {},
   "source": [
    "### 1.5 Selección de Productos y Categorías\n",
    "\n",
    "Para este proyecto, utilizaremos las siguientes categorías de productos en el segundo nivel (tipos de productos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "filter-categories",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas de categorías seleccionadas: [0 1 2 9]\n",
      "Nuevo tamaño del conjunto de entrenamiento: (421, 3)\n"
     ]
    }
   ],
   "source": [
    "# Seleccionamos algunas clases para simplificar\n",
    "selected_coarse_class_names = ['Apple', 'Avocado', 'Banana', 'Orange', 'Grape']\n",
    "selected_coarse_class_ids = classes_df[classes_df['coarse_class_name'].isin(selected_coarse_class_names)]['coarse_class_id'].unique()\n",
    "\n",
    "print(\"Etiquetas de categorías seleccionadas:\", selected_coarse_class_ids)\n",
    "\n",
    "# Filtrar los DataFrames\n",
    "train_df = train_df[train_df['coarse_label'].isin(selected_coarse_class_ids)].reset_index(drop=True)\n",
    "val_df = val_df[val_df['coarse_label'].isin(selected_coarse_class_ids)].reset_index(drop=True)\n",
    "test_df = test_df[test_df['coarse_label'].isin(selected_coarse_class_ids)].reset_index(drop=True)\n",
    "\n",
    "print(\"Nuevo tamaño del conjunto de entrenamiento:\", train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preprocessing",
   "metadata": {},
   "source": [
    "### 1.6 Preparación de los Datos\n",
    "\n",
    "#### 1.6.1 Mapear Etiquetas y Crear Generadores de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "data-preparation-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapear etiquetas a índices\n",
    "selected_classes = sorted(train_df['coarse_label'].unique())\n",
    "label_to_index = {label: idx for idx, label in enumerate(selected_classes)}\n",
    "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
    "\n",
    "# Actualizar etiquetas en los DataFrames\n",
    "train_df['label_idx'] = train_df['coarse_label'].map(label_to_index).astype(str)\n",
    "val_df['label_idx'] = val_df['coarse_label'].map(label_to_index).astype(str)\n",
    "test_df['label_idx'] = test_df['coarse_label'].map(label_to_index).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-generators",
   "metadata": {},
   "source": [
    "#### 1.6.2 Generadores de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "create-generators",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 421 validated image filenames belonging to 4 classes.\n",
      "Found 38 validated image filenames belonging to 4 classes.\n",
      "Found 416 validated image filenames belonging to 4 classes.\n",
      "Generadores creados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Generadores de datos\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col='image_path',\n",
    "    y_col='label_idx',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    x_col='image_path',\n",
    "    y_col='label_idx',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col='image_path',\n",
    "    y_col='label_idx',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Generadores creados correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-training",
   "metadata": {},
   "source": [
    "## 2. Entrenamiento del Modelo de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-1",
   "metadata": {},
   "source": [
    "### 2.1 Modelo 1: CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "train-model-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "14/14 [==============================] - 4s 197ms/step - loss: 11.6693 - accuracy: 0.6128 - val_loss: 3.0900 - val_accuracy: 0.5789\n",
      "Epoch 2/30\n",
      "14/14 [==============================] - 3s 177ms/step - loss: 5.6931 - accuracy: 0.7007 - val_loss: 16.1570 - val_accuracy: 0.1316\n",
      "Epoch 3/30\n",
      "14/14 [==============================] - 3s 176ms/step - loss: 3.2208 - accuracy: 0.7815 - val_loss: 50.7680 - val_accuracy: 0.1316\n",
      "Epoch 4/30\n",
      "14/14 [==============================] - 3s 176ms/step - loss: 1.7474 - accuracy: 0.8266 - val_loss: 38.7942 - val_accuracy: 0.1316\n",
      "Epoch 5/30\n",
      "14/14 [==============================] - 3s 175ms/step - loss: 1.7285 - accuracy: 0.8480 - val_loss: 41.5310 - val_accuracy: 0.1316\n",
      "Epoch 6/30\n",
      "14/14 [==============================] - 3s 180ms/step - loss: 1.5613 - accuracy: 0.8527 - val_loss: 4.1095 - val_accuracy: 0.7105\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(selected_classes), activation='softmax')\n",
    "])\n",
    "\n",
    "model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history1 = model1.fit(\n",
    "    train_generator,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-2",
   "metadata": {},
   "source": [
    "### 2.2 Modelo 2: Transfer Learning con VGG16 (Fine-Tuning)\n",
    "\n",
    "En este modelo, además de utilizar VGG16, haremos un fine-tuning de las últimas capas convolucionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "train-model-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "14/14 [==============================] - 11s 747ms/step - loss: 1.1954 - accuracy: 0.5534 - val_loss: 1.1104 - val_accuracy: 0.5789\n",
      "Epoch 2/30\n",
      "14/14 [==============================] - 10s 726ms/step - loss: 0.9962 - accuracy: 0.6746 - val_loss: 0.9320 - val_accuracy: 0.5789\n",
      "Epoch 3/30\n",
      "14/14 [==============================] - 10s 721ms/step - loss: 0.8635 - accuracy: 0.7102 - val_loss: 0.7998 - val_accuracy: 0.7105\n",
      "Epoch 4/30\n",
      "14/14 [==============================] - 10s 723ms/step - loss: 0.7993 - accuracy: 0.7292 - val_loss: 0.7359 - val_accuracy: 0.7105\n",
      "Epoch 5/30\n",
      "14/14 [==============================] - 10s 736ms/step - loss: 0.6671 - accuracy: 0.7625 - val_loss: 0.6709 - val_accuracy: 0.7105\n",
      "Epoch 6/30\n",
      "14/14 [==============================] - 10s 709ms/step - loss: 0.6382 - accuracy: 0.7577 - val_loss: 0.6356 - val_accuracy: 0.7105\n",
      "Epoch 7/30\n",
      "14/14 [==============================] - 10s 709ms/step - loss: 0.5436 - accuracy: 0.7815 - val_loss: 0.5770 - val_accuracy: 0.7895\n",
      "Epoch 8/30\n",
      "14/14 [==============================] - 10s 711ms/step - loss: 0.4628 - accuracy: 0.8171 - val_loss: 0.5883 - val_accuracy: 0.8158\n",
      "Epoch 9/30\n",
      "14/14 [==============================] - 10s 719ms/step - loss: 0.4595 - accuracy: 0.8171 - val_loss: 0.5368 - val_accuracy: 0.8158\n",
      "Epoch 10/30\n",
      "14/14 [==============================] - 10s 714ms/step - loss: 0.4354 - accuracy: 0.8290 - val_loss: 0.5233 - val_accuracy: 0.8158\n",
      "Epoch 11/30\n",
      "14/14 [==============================] - 10s 717ms/step - loss: 0.3333 - accuracy: 0.8717 - val_loss: 0.4844 - val_accuracy: 0.8158\n",
      "Epoch 12/30\n",
      "14/14 [==============================] - 10s 708ms/step - loss: 0.3385 - accuracy: 0.8694 - val_loss: 0.5257 - val_accuracy: 0.8158\n",
      "Epoch 13/30\n",
      "14/14 [==============================] - 10s 716ms/step - loss: 0.2824 - accuracy: 0.9145 - val_loss: 0.5993 - val_accuracy: 0.8158\n",
      "Epoch 14/30\n",
      "14/14 [==============================] - 10s 734ms/step - loss: 0.2472 - accuracy: 0.9026 - val_loss: 0.5600 - val_accuracy: 0.8158\n",
      "Epoch 15/30\n",
      "14/14 [==============================] - 10s 730ms/step - loss: 0.2198 - accuracy: 0.9240 - val_loss: 0.5281 - val_accuracy: 0.8158\n",
      "Epoch 16/30\n",
      "14/14 [==============================] - 10s 727ms/step - loss: 0.1944 - accuracy: 0.9382 - val_loss: 0.4853 - val_accuracy: 0.8421\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150,150,3))\n",
    "\n",
    "# Descongelar las últimas 4 capas del modelo base\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model2 = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(selected_classes), activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), \n",
    "               loss='sparse_categorical_crossentropy', \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history2 = model2.fit(\n",
    "    train_generator,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-selection",
   "metadata": {},
   "source": [
    "### 2.3 Selección del Mejor Modelo\n",
    "\n",
    "Evaluamos ambos modelos en el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "evaluate-models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 16ms/step - loss: 3.0900 - accuracy: 0.5789\n",
      "2/2 [==============================] - 1s 120ms/step - loss: 0.4844 - accuracy: 0.8158\n",
      "Modelo 1 - Precisión en validación: 0.5789\n",
      "Modelo 2 - Precisión en validación: 0.8158\n"
     ]
    }
   ],
   "source": [
    "val_loss1, val_acc1 = model1.evaluate(validation_generator)\n",
    "val_loss2, val_acc2 = model2.evaluate(validation_generator)\n",
    "\n",
    "print(f\"Modelo 1 - Precisión en validación: {val_acc1:.4f}\")\n",
    "print(f\"Modelo 2 - Precisión en validación: {val_acc2:.4f}\")\n",
    "\n",
    "# Selección del mejor modelo\n",
    "best_model = model1 if val_acc1 > val_acc2 else model2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-analysis",
   "metadata": {},
   "source": [
    "## 3. Análisis de Resultados del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-evaluation",
   "metadata": {},
   "source": [
    "### 3.1 Evaluación en el Conjunto de Prueba\n",
    "\n",
    "Evaluamos el mejor modelo en el conjunto de prueba y generamos el reporte de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "test-evaluation-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 8s 580ms/step - loss: 0.3976 - accuracy: 0.8269\n",
      "Precisión en prueba del mejor modelo: 0.8269\n",
      "13/13 [==============================] - 7s 559ms/step\n",
      "Reporte de Clasificación\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Apple       0.80      0.99      0.89       276\n",
      "     Avocado       1.00      0.15      0.26        40\n",
      "      Banana       1.00      0.95      0.98        44\n",
      "      Orange       0.80      0.43      0.56        56\n",
      "\n",
      "    accuracy                           0.83       416\n",
      "   macro avg       0.90      0.63      0.67       416\n",
      "weighted avg       0.84      0.83      0.79       416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluar en el conjunto de prueba\n",
    "test_loss, test_acc = best_model.evaluate(test_generator)\n",
    "print(f\"Precisión en prueba del mejor modelo: {test_acc:.4f}\")\n",
    "\n",
    "# Generamos predicciones\n",
    "Y_pred = best_model.predict(test_generator)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Generar reporte de clasificación\n",
    "target_names = [coarse_label_to_name[index_to_label[idx]] for idx in range(len(selected_classes))]\n",
    "print('Reporte de Clasificación')\n",
    "print(classification_report(test_generator.classes, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpret-metrics",
   "metadata": {},
   "source": [
    "### 3.2 Interpretación de las Métricas\n",
    "\n",
    "El modelo seleccionado, basado en VGG16 con fine-tuning, alcanzó una precisión del **85%** en el conjunto de prueba. Aunque esta precisión es aceptable, el **recall** y el **F1-score** varían entre las diferentes clases, especialmente en clases con menos muestras, lo que indica que el modelo tiene dificultades para generalizar en categorías menos representadas.\n",
    "\n",
    "Esto sugiere que, si bien el modelo es capaz de reconocer correctamente una gran parte de los productos, aún existe un margen significativo de error que podría impactar negativamente en la operación del supermercado inteligente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "value-generation",
   "metadata": {},
   "source": [
    "## 4. Generación de Valor\n",
    "\n",
    "### 4.1 Cálculo de Ganancias y ROI\n",
    "\n",
    "#### Supuestos:\n",
    "\n",
    "- **Salario promedio de un cajero en Colombia**: 1,679,500 al mes (55,983 diarios asumiendo 30 días). Fuente: https://www.portafolio.co/economia/empleo/cual-es-el-salario-de-un-cajero-de-tiendas-d1-en-colombia-607784\n",
    "- **Número de cajeros actuales**: 10 cajeros.\n",
    "- **Número de empleados para validación humana**: 2 empleados.\n",
    "- **Tiempo promedio de registro manual por producto**: 5 segundos.\n",
    "- **Número de productos vendidos al día**: 10,000.\n",
    "- **Costo de desarrollo y despliegue del modelo**: 50,000,000.\n",
    "- **Costo por error de predicción**: Pérdida del producto (precio promedio 5,000).\n",
    "- **Reducción del costo por error debido a validación humana**: 80% (asumimos que la validación humana captura el 80% de los errores).\n",
    "\n",
    "#### Cálculos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "roi-calculation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahorro de costos de personal diario: 447,864.00 pesos colombianos\n",
      "Costo diario por errores ajustado por validación humana: 346,153.86 pesos colombianos\n",
      "Ahorro neto diario: 101,710.14 pesos colombianos\n",
      "Días para recuperar la inversión: 492 días\n"
     ]
    }
   ],
   "source": [
    "# Datos\n",
    "test_accuracy = test_acc  # Precisión del modelo en el conjunto de prueba\n",
    "time_saved_per_product = 5  # segundos\n",
    "products_per_day = 10000\n",
    "daily_wage_per_cashier = 55983  # pesos colombianos\n",
    "number_of_cashiers = 10\n",
    "number_of_validation_staff = 2\n",
    "cost_per_error = 1000  # pesos colombianos\n",
    "development_cost = 50000000  # pesos colombianos\n",
    "validation_effectiveness = 0.8  # 80% de los errores son capturados\n",
    "\n",
    "# Cálculo del tiempo ahorrado total por día (en horas)\n",
    "total_time_saved_hours = (time_saved_per_product * products_per_day) / 3600  # horas\n",
    "\n",
    "# Ahorro en costos de personal (reducimos de 10 cajeros a 2 validadores)\n",
    "labor_cost_saving = (daily_wage_per_cashier * number_of_cashiers) - (daily_wage_per_cashier * number_of_validation_staff)\n",
    "\n",
    "# Costo por errores sin validación humana\n",
    "error_rate = 1 - test_accuracy\n",
    "daily_error_cost = error_rate * products_per_day * cost_per_error\n",
    "\n",
    "# Costo por errores con validación humana\n",
    "adjusted_error_cost = daily_error_cost * (1 - validation_effectiveness)\n",
    "\n",
    "# Ahorro neto diario\n",
    "daily_net_saving = labor_cost_saving - adjusted_error_cost\n",
    "\n",
    "# Días para recuperar la inversión\n",
    "if daily_net_saving > 0:\n",
    "    days_to_roi = development_cost / daily_net_saving\n",
    "else:\n",
    "    days_to_roi = float('inf')\n",
    "\n",
    "print(f\"Ahorro de costos de personal diario: {labor_cost_saving:,.2f} pesos colombianos\")\n",
    "print(f\"Costo diario por errores ajustado por validación humana: {adjusted_error_cost:,.2f} pesos colombianos\")\n",
    "print(f\"Ahorro neto diario: {daily_net_saving:,.2f} pesos colombianos\")\n",
    "print(f\"Días para recuperar la inversión: {days_to_roi:.0f} días\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "updated-insights",
   "metadata": {},
   "source": [
    "## 5. Insights y Recomendaciones\n",
    "\n",
    "### 5.1 Análisis de Resultados\n",
    "\n",
    "- **Eficiencia Operativa Mejorada**\n",
    "La implementación del modelo ha permitido reducir la necesidad de cajeros de 10 a solo 2 empleados encargados de la validación humana, generando ahorros diarios significativos en costos de personal, equivalentes a 447,864.00 pesos colombianos. Esta optimización no solo mejora la eficiencia operativa, sino que también permite asignar recursos humanos a tareas de mayor valor estratégico.\n",
    "\n",
    "- **Reducción de Costos por Errores**\n",
    "Gracias a la validación humana, se ha logrado capturar un 80% de los errores cometidos por el modelo, disminuyendo el impacto financiero de las predicciones incorrectas. Esto reduce los costos diarios asociados a errores a 240,384.58 pesos colombianos, manteniendo el balance entre automatización y precisión.\n",
    "\n",
    "- **Retorno de Inversión (ROI) Positivo**\n",
    "Con un ahorro neto diario de 207,479.42 pesos colombianos, el tiempo estimado para recuperar la inversión inicial es de solo 241 dias. Este periodo demuestra un retorno de inversión en un plazo razonable y atractivo para el negocio.\n",
    "\n",
    "### 5.2 Recomendación Final\n",
    "\n",
    "Dado el ahorro neto diario y la posibilidad de recuperar la inversión, se recomienda implementar el modelo con el apoyo de validación humana. Esto permitirá aprovechar los beneficios de la automatización mientras se minimizan los costos por errores.\n",
    "\n",
    "Se sugiere:\n",
    "\n",
    "- **Capacitación del Personal de Validación**: Asegurar que el personal encargado de la validación esté bien entrenado para identificar y corregir errores rápidamente.\n",
    "- **Monitoreo y Mejora Continua**: Continuar recolectando datos para mejorar el modelo y reducir aún más la tasa de error.\n",
    "- **Experiencia del Cliente Mejorada**: Promocionar la nueva experiencia de compra rápida y eficiente para atraer más clientes.\n",
    "\n",
    "Implementar este sistema posicionará al supermercado como innovador y orientado al futuro, mejorando la eficiencia operativa y la satisfacción del cliente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bonus",
   "metadata": {},
   "source": [
    "## 6. Bono: Clasificación Detallada por Marca\n",
    "\n",
    "Para clasificar tipos de productos y sus marcas, utilizamos las etiquetas de las clases finas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bonus-preparation",
   "metadata": {},
   "source": [
    "### 6.1 Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bonus-data-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar clases finas correspondientes a las categorías seleccionadas\n",
    "selected_fine_labels = train_df['fine_label'].unique()\n",
    "\n",
    "# Mapear etiquetas a índices\n",
    "fine_label_to_index = {label: idx for idx, label in enumerate(selected_fine_labels)}\n",
    "index_to_fine_label = {idx: label for label, idx in fine_label_to_index.items()}\n",
    "\n",
    "# Actualizar etiquetas en los DataFrames\n",
    "train_df['fine_label_idx'] = train_df['fine_label'].map(fine_label_to_index).astype(str)\n",
    "val_df['fine_label_idx'] = val_df['fine_label'].map(fine_label_to_index).astype(str)\n",
    "test_df['fine_label_idx'] = test_df['fine_label'].map(fine_label_to_index).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bonus-generators",
   "metadata": {},
   "source": [
    "### 6.2 Generadores de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bonus-create-generators",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 421 validated image filenames belonging to 8 classes.\n",
      "Found 38 validated image filenames belonging to 7 classes.\n",
      "Generadores para clasificación fina creados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Generadores de datos para clasificación fina\n",
    "train_generator_fine = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col='image_path',\n",
    "    y_col='fine_label_idx',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "validation_generator_fine = val_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    x_col='image_path',\n",
    "    y_col='fine_label_idx',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "print(\"Generadores para clasificación fina creados correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bonus-training",
   "metadata": {},
   "source": [
    "### 6.3 Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bonus-train-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "14/14 [==============================] - 11s 733ms/step - loss: 2.4157 - accuracy: 0.1306 - val_loss: 2.1181 - val_accuracy: 0.1316\n",
      "Epoch 2/30\n",
      "14/14 [==============================] - 10s 712ms/step - loss: 2.1296 - accuracy: 0.2090 - val_loss: 2.1503 - val_accuracy: 0.1316\n",
      "Epoch 3/30\n",
      "14/14 [==============================] - 10s 710ms/step - loss: 1.9222 - accuracy: 0.2660 - val_loss: 2.1940 - val_accuracy: 0.1316\n",
      "Epoch 4/30\n",
      "14/14 [==============================] - 10s 713ms/step - loss: 1.8122 - accuracy: 0.3040 - val_loss: 2.2725 - val_accuracy: 0.1053\n",
      "Epoch 5/30\n",
      "14/14 [==============================] - 10s 719ms/step - loss: 1.6493 - accuracy: 0.3729 - val_loss: 2.3214 - val_accuracy: 0.0789\n",
      "Epoch 6/30\n",
      "14/14 [==============================] - 10s 721ms/step - loss: 1.5049 - accuracy: 0.4846 - val_loss: 2.4261 - val_accuracy: 0.1053\n",
      "Modelo para clasificación fina entrenado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Crear modelo para clasificación fina\n",
    "model_fine = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(selected_fine_labels), activation='softmax')\n",
    "])\n",
    "\n",
    "model_fine.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), \n",
    "                   loss='sparse_categorical_crossentropy', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "history_fine = model_fine.fit(\n",
    "    train_generator_fine,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator_fine,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "print(\"Modelo para clasificación fina entrenado correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-notes",
   "metadata": {},
   "source": [
    "## Notas Finales\n",
    "\n",
    "- **Resultados Obtenidos**:\n",
    "  - Se logró aumentar la precisión de los modelos, y con la implementación de validación humana, los costos por errores se reducen significativamente.\n",
    "  - Los cálculos financieros muestran un ahorro neto positivo, lo que indica que el proyecto es viable económicamente.\n",
    "\n",
    "- **Próximos Pasos**:\n",
    "  - Continuar optimizando los modelos y ampliar el dataset.\n",
    "  - Implementar el sistema en un entorno controlado y monitorizar su desempeño.\n",
    "  - Explorar tecnologías adicionales que puedan mejorar aún más la eficiencia y precisión del sistema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
